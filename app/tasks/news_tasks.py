"""æ–°é—»ç›¸å…³çš„å®šæ—¶ä»»åŠ¡"""

from datetime import date, timedelta

from loguru import logger

from app.core.database import async_session
from app.core.scheduler import scheduler_manager
from app.services.news_service import NewsService


def backfill_embeddings_for_week(start_date: date, end_date: date) -> dict:
    """è¡¥å……æŒ‡å®šæ—¥æœŸèŒƒå›´çš„ embeddingï¼ˆå¤ç”¨ backfill_embeddings é€»è¾‘ï¼‰"""
    from app.agents.rag.news_rag import (
        count_news_articles,
        fetch_news_articles,
        get_embeddings,
        get_vector_store,
        news_to_documents,
    )

    stats = {"added": 0, "skipped": 0, "failed": 0}

    total_in_db = count_news_articles(start_date, end_date)
    if total_in_db == 0:
        logger.info("è¯¥æ—¥æœŸèŒƒå›´å†…æ— æ–‡ç« éœ€è¦å¤„ç† embedding")
        return stats

    vector_store = get_vector_store()
    collection = vector_store._collection
    results = collection.get(include=["metadatas"])
    indexed_urls = set()
    if results and results.get("metadatas"):
        indexed_urls = {m["url"] for m in results["metadatas"] if m.get("url")}

    logger.info(f"æ•°æ®åº“ä¸­æœ‰ {total_in_db} ç¯‡æ–‡ç« ï¼Œå·²ç´¢å¼• {len(indexed_urls)} æ¡")

    embeddings = get_embeddings()
    vector_store = get_vector_store(embeddings)

    offset = 0
    fetch_size = 100
    embedding_batch_size = 10

    while True:
        articles = fetch_news_articles(
            limit=fetch_size,
            offset=offset,
            start_date=start_date,
            end_date=end_date,
        )
        if not articles:
            break

        new_articles = [a for a in articles if a["url"] not in indexed_urls]
        stats["skipped"] += len(articles) - len(new_articles)

        if new_articles:
            documents = news_to_documents(new_articles)
            for i in range(0, len(documents), embedding_batch_size):
                batch = documents[i : i + embedding_batch_size]
                try:
                    vector_store.add_documents(batch)
                    stats["added"] += len(batch)
                    for doc in batch:
                        indexed_urls.add(doc.metadata["url"])
                except Exception as e:
                    logger.error(f"ç´¢å¼•å¤±è´¥: {e}")
                    stats["failed"] += len(batch)

        offset += fetch_size

    return stats


async def daily_maintenance_task():
    """æ¯æ—¥ç»´æŠ¤ä»»åŠ¡ï¼šæ£€æµ‹å¹¶è¡¥å……è¿‡å»ä¸€å‘¨çš„æ–°é—»æ•°æ®å’Œ embedding

    ä»»åŠ¡è¯´æ˜ï¼š
    - æ¯å¤© 05:00 è‡ªåŠ¨æ‰§è¡Œ
    - æ£€æŸ¥è¿‡å» 7 å¤©çš„æ–°é—»æ•°æ®ï¼Œç¼ºå¤±åˆ™è¡¥å……
    - æ£€æŸ¥ embedding ç´¢å¼•ï¼Œç¼ºå¤±åˆ™è¡¥å……
    - å®ç° 0 äººå·¥ç»´æŠ¤æˆæœ¬
    """
    try:
        logger.info("=" * 60)
        logger.info("ğŸ¤– æ¯æ—¥ç»´æŠ¤ä»»åŠ¡è§¦å‘")
        logger.info("=" * 60)

        today = date.today()
        start_date = today - timedelta(days=7)

        # ç¬¬ä¸€æ­¥ï¼šè¡¥å……æ–°é—»æ•°æ®
        logger.info("ğŸ“° æ­¥éª¤ 1/2ï¼šæ£€æŸ¥å¹¶è¡¥å……æ–°é—»æ•°æ®")
        news_stats = {"success": 0, "skipped": 0, "failed": 0}

        for i in range(7):
            target_date = today - timedelta(days=i)
            async with async_session() as session:
                news_service = NewsService(session)
                existing_count = await news_service.get_news_count_by_date(target_date)

                if existing_count > 0:
                    logger.info(f"  {target_date} å·²æœ‰ {existing_count} æ¡ï¼Œè·³è¿‡")
                    news_stats["skipped"] += 1
                else:
                    try:
                        articles = await news_service.fetch_and_save_daily_news(
                            target_date
                        )
                        logger.info(f"  {target_date} é‡‡é›† {len(articles)} æ¡")
                        news_stats["success"] += 1
                    except Exception as e:
                        logger.error(f"  {target_date} å¤±è´¥: {e}")
                        news_stats["failed"] += 1

        logger.info(
            f"æ–°é—»è¡¥å……å®Œæˆ - æˆåŠŸ: {news_stats['success']} | "
            f"è·³è¿‡: {news_stats['skipped']} | å¤±è´¥: {news_stats['failed']}"
        )

        # ç¬¬äºŒæ­¥ï¼šè¡¥å…… embedding
        logger.info("ğŸ” æ­¥éª¤ 2/2ï¼šæ£€æŸ¥å¹¶è¡¥å…… embedding")
        embed_stats = backfill_embeddings_for_week(start_date, today)
        logger.info(
            f"Embedding è¡¥å……å®Œæˆ - æ–°å¢: {embed_stats['added']} | "
            f"è·³è¿‡: {embed_stats['skipped']} | å¤±è´¥: {embed_stats['failed']}"
        )

        logger.info("=" * 60)
        logger.info("âœ“ æ¯æ—¥ç»´æŠ¤ä»»åŠ¡å®Œæˆ")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"âŒ æ¯æ—¥ç»´æŠ¤ä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)


def register_news_tasks():
    """æ³¨å†Œæ–°é—»ç›¸å…³çš„å®šæ—¶ä»»åŠ¡

    ä»»åŠ¡è°ƒåº¦è¯´æ˜ï¼š
    - æ¯å¤© 05:00 æ‰§è¡Œæ¯æ—¥ç»´æŠ¤ä»»åŠ¡
    - è‡ªåŠ¨æ£€æµ‹å¹¶è¡¥å……è¿‡å»ä¸€å‘¨çš„æ–°é—»æ•°æ®å’Œ embedding
    """
    scheduler = scheduler_manager.scheduler

    scheduler.add_job(
        daily_maintenance_task,
        trigger="cron",
        hour=5,
        minute=0,
        id="daily_maintenance_task",
        name="æ¯æ—¥ç»´æŠ¤ä»»åŠ¡ï¼ˆæ–°é—»+Embeddingï¼‰",
        replace_existing=True,
    )
    logger.info("âœ“ å·²æ³¨å†Œä»»åŠ¡ï¼šæ¯æ—¥ç»´æŠ¤ä»»åŠ¡ï¼ˆæ¯å¤© 05:00ï¼‰")

    logger.info("=" * 60)
    logger.info("ğŸ¯ æ–°é—»å®šæ—¶ä»»åŠ¡æ³¨å†Œå®Œæˆ")
    logger.info("=" * 60)


if __name__ == "__main__":
    import asyncio

    asyncio.run(daily_maintenance_task())
